{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Data Analysis\n",
    "\n",
    "Content: \n",
    "\n",
    "- No Free Lunch Theorem\n",
    "- Categories of Data Analysis Methods\n",
    "- Foundations of Machine Learning\n",
    "\n",
    "## No Free Lunch Theorem (NFL)\n",
    "\n",
    "Before we dive into the algorithms for the creation of models about the data, we need some foundations. The first foundation is a fundamental theorem about optimization algorithms, the *No Free Lunch Theorem* (NFL). This now gets a bit theoretical, if you are just interested into the the (very important!) consequences of the theorem on data modeling, just skip to the colloquial definition. The mathematical formulation of the NFL is the following. \n",
    "\n",
    "> **No Free Lunch Theorem:**\n",
    ">\n",
    ">Let $d_m^y$ be ordered sets of size $m$ with cost values $y \\in Y$. Let $f: X \\to Y$ a function that should be optimized. Let $P(d_m^y|f, m, a)$ the conditional probability of getting $d_m^y$ by $m$ times running the algorithm $a$ on the function $f$.\n",
    ">\n",
    "> For any pair of algorithms $a_1$ and $a_2$ to following equations holds:\n",
    "> \n",
    "> $$\\sum_f P(d_m^y|f, m, a_1) = \\sum_f P(d_m^y|f, m, a_2)$$\n",
    "\n",
    "A proof for the theorem can be [found in the literature](TODO). The equation of the NFL tells us the the sum of probabilities of getting certain costs is equal if all functions $f$ are considered for any two algorithms. The meaning of this can be summarized as follows. \n",
    "\n",
    "> **No Free Lunch Theorem (Colloquial):**\n",
    ">\n",
    "> All algorithms are equal, when all possible problems are considered. \n",
    "\n",
    "This is counter-intuitive, because we observe notable differences between different algorithms in practice. However, the NFL does not prohibit that an algorithms is better for some functions $f$ - i.e., for some problems. But if an algorithm is better than the average algorithm for some problems, the NFL tells us that the algorithm must be worse than the average algorithm for other problems. Thus, the true consequence of the NFL is that *there is not one algorithm that is optimal for all problems*. Instead, different algorithms are better on different for different problems. That is also the reason for the name: there is no \"free lunch\", i.e., one algorithm for all problems. Instead, data scientist must earn their lunch, i.e., find a suitable algorithm. \n",
    "\n",
    "This means that having detailed knowledge about one algorithm is not sufficient. Data scientists need a whole toolbox of algorithms. They must be able to select a suitable algorithm, based on the data, their knowledge about the problem, and their experience based on working with different algorithms. Over the time, data scientists gather experience with more and more algorithms and become experts in selecting suitable algorithms for a given problem. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

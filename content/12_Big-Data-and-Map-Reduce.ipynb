{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Processing\n",
    "\n",
    "## Distributed Computing\n",
    "\n",
    "As we know from [Chapter 1](01_Introduction), Big Data is data whose volume, velocity, and variety requires *innovative forms of information processing*. In this chapter, we want to discuss in greater detail why this is the case and how Big Data can processed. \n",
    "\n",
    "The foundation of any large computational effort is *parallelism*. There is a famous quote from computer science pioneer Grace Hopper: \"*In pioneer days they used oxen for heavy pulling, and when one ox couldn't budge a log, they didn't try to grow a larger ox. We should be trying for bigger computers, but for more systems of computers*. In other words, large tasks can only be solved by pooling resources. There are three general methods for the parallelization of computational tasks. \n",
    "\n",
    "### Parallel Programming Models\n",
    "\n",
    "First approach *message passing* tasks are executed independently in isolated environments. Whenever these tasks want to communicate, they send messages to each other. This way, the tasks can exchange data between each other, e.g., because the data is required by different parts of the computation. This communication can be done locally on one physical machine, by using the provided functions by the operating system, or remotely in a distributed environment by communicating via the network. \n",
    "\n",
    "The second approach is *shared memory*. In this case, the computational tasks are not performed in isolated environments, but share a common address space in the memory, i.e., they can read and write the same variables. Interactions between the tasks happens by updating the values of variables in the shared memory. Sharing memory within a single physical machine is directly supported by the operating system, and may even be a property of the model for parallelization (threads share the same memory, processes not). Sharing memory across different physical machines is also possible, e.g., via network attached storage of other networking solutions, but usually has some communication overhead. \n",
    "\n",
    "The third approach is *data parallelism*. Similar to message passing, tasks are executed independently in isolated environments. The difference to message passing is that the tasks do not need to communicate with each other, because the solution of the computational tasks does not require intermediary results of other tasks. Thus, the application of data parallelism is limited to problems where this strong decoupling of tasks is possible. Such problems are also called *embarrassingly parallel*. \n",
    "\n",
    "### Distributed Computing for Data Analysis\n",
    "\n",
    "Since Big Data is to large to compute or store on single physical machines, we need a distributed environment for computations that involve Big Data. Before computational centers started to account for Big Data, the architecture of such a *compute cluster* was similar to the outline below. \n",
    "\n",
    "<img src=\"images/computing_architectures.png\" alt=\"Distributed Computing Outline\" style=\"width: 600px;\"/>\n",
    "\n",
    "There is a layer for data storage and a layer for computations. Both are using different *nodes* in the compute cluster. Each node is a physical machine. Data storage nodes must provide fast storage (latency, throughput, or both), but do not require much computational power. This is usually implement in a database or a *storage area network* (SAN). Vice versa, compute nodes must provide the computational power through CPUs (and possibly GPUs) and a sufficient amount of memory, local storage is less important and often only used for caching and the installation of software. A user of such a system submits jobs to a job queue to gain insights. For the analysis of data, this means that the data is stored in the database or SAN and then accessed by the compute nodes to generate the desired results of the analysis, from which the data scientists can get insights. \n",
    "\n",
    "All three parallelization modes we discussed above can be implemented in such a traditional distributed compute cluster. However, none of these approaches is suitable for big data applications in such a compute cluster. Message passing and shared memory have the biggest scalability problems. \n",
    "\n",
    "<img src=\"images/mpi_sm.png\" alt=\"Distributed Computing with MPI/SM\" style=\"width: 600px;\"/>\n",
    "\n",
    "Since it is unclear which parts of the data are required by the different parallel tasks, it is possibly that every compute node must load all data. While this is not a problem for small data sets, this does not scale with large data sets. Imagine that Terabytes, or even Petabytes of data would have to be copied regularly over the network. The transfer of the data would block the execution of the analysis and the compute nodes would be mostly idle, waiting for data. This does not even account for additional network traffic due to the communication between the tasks. \n",
    "\n",
    "Data parallelization fares a bit better, but also does not scale. \n",
    "\n",
    "<img src=\"images/data_parallelism.png\" alt=\"Distributed Computing with Data Parallelism\" style=\"width: 600px;\"/>\n",
    "\n",
    "The advantage of message passing and shared memory is that only parts of the data must be copied to each compute node. While this decreases the stress on the network, all data must still be transfered over network. Thus, data parallelization can handle larger amounts of data than message passing and shared memory, at some point the amount of data becomes to large for the transfer via the network. \n",
    "\n",
    "### Data Locality\n",
    "\n",
    "We see that there is a fundamental problem with traditional distributed computing for big data, which is why we need the *innovative forms of information processing*. The solution is actually quite simple: if the problem is that we cannot copy our data over the network, we must change our architecture such that avoid that. The straightforward way to achieve this is to break the separation of the storage layer from the compute layer: all nodes both store data and can perform computations on that data. \n",
    "\n",
    "<img src=\"images/data_locality.png\" alt=\"Distributed Computing with Data Locality\" style=\"width: 400px;\"/>\n",
    "\n",
    "In the following, we explain how this is implemented in practice. We discuss the MapReduce programming model that became the defacto standard for Big Data applications. Then, we show Apache Hadoop and Apache Spark to demonstrate how the distributed computing with Big Data is implemented."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce\n",
    "\n",
    "The MapReduce paradigm for data the data parallelization to enable BigData processing was [published by Google in 2004](https://doi.org/10.1145/1327452.1327492). The general idea is to describe computations using two different kinds of functions: *map* functions and *reduce* functions. Both functions work with *key-value pair*. Map functions implement the embarrassingly parallel part of algorithms, reduce functions aggregate the results. The concept of map functions and reduce functions is not unique to MapReduce, but a general concept that can be found in many functional programming languages. To enable Big Data, MapReduce introduces a third function, the *shuffle*. The only task of the *shuffle* is to arrange intermediate results, i.e., to facilitate the communication between the map and reduce functions. The following figure gives an overview of the dataflow of MapReduce. \n",
    "\n",
    "<img src=\"images/map_reduce_complete.png\" alt=\"Outline of MapReduce\" style=\"width: 600px;\"/>\n",
    "\n",
    "### map()\n",
    "\n",
    "The map function gets initial key-value pairs. These are, e.g., read from the the distributed storage or the result of a prior computation using MapReduce. The map function than performs a computation on a *single* key-value pair and stores the results in new key-value pairs. Because the map function only gets a single key-value pair as input, data parallelization is trivial: theoretically, the map function could run in parallel for all key-value pairs without any problem. The map function is defined as\n",
    "\n",
    "$$map(f_{map}, <key1, value1>) \\rightarrow list(<key2, value2>)$$\n",
    "\n",
    "where $f_{map}$ is a *user-defined function* (UDF) defined by the user of the MapReduce framework. The UDF defines the computation, i.e., how the input key-value pair is transformed into the list of output key-value pairs. Depending on the UDF $f_{map}$, the input keys and output keys could be same or different. While the general concept of MapReduce does not have any restrictions on the type and values of the keys, implementations of MapReduce may restrict this. For example, in the initial implementation of MapReduce by Google, all keys and values were strings and users of MapReduce were expected to convert the types within the map and reduce functions, if required.\n",
    "\n",
    "### shuffle()\n",
    "\n",
    "The key-value pairs computed by the map function are organized by the shuffle function, such that the data is grouped by the key. These are then organized by the shuffle and grouped by their keys. Thus, we have\n",
    "\n",
    "$$shuffle(list<key2, value2>) \\rightarrow list(<key2, list(value2)>),$$\n",
    "\n",
    "i.e., a list of values per key. Often, these data from shuffle is sorted by key, because this can sometimes improve the efficiency of subsequent tasks. The shuffling is often invisible to the user performed in the background by the MapReduce framework. \n",
    "\n",
    "### reduce()\n",
    "\n",
    "The reduce function operates on all values for a given key and aggregates the data into a single result per key.  The reduce function is defined as\n",
    "\n",
    "$$reduce(f_{reduce}, <key2, list(value2)>) \\rightarrow value3$$\n",
    "\n",
    "where $f_{reduce}$ is a UDF. The UDF $f_{reduce}$ performs the reduction to a single value for one key and gets as input the key and the related list of values. Similar as for the map function, there is no restriction on the type or the values that are generated. Depending on the task, the output could, e.g.,  be key value pairs, integers, or textual data. \n",
    "\n",
    "### Word Count with MapReduce\n",
    "\n",
    "The concept of MapReduce is relatively abstract, unless you are used to functional programming. How MapReduce works becomes clearer with an example. The \"Hello World\" of MapReduce is the word count, i.e., using MapReduce to count how often each word occurs in a text. This example is both practically relevant, e.g., to create a bag-of-words, and well suited to demonstrate how MapReduce works. \n",
    "\n",
    "We use the following text as example: \n",
    "\n",
    "```\n",
    "What is your name?\n",
    "The name is Bond, James Bond.\n",
    "```\n",
    "\n",
    "Our data is stored in a text file with one line per sentence. Our initial keys are the line numbers, our initial values the text in the lines. Thus, we start with these key-value pairs.\n",
    "\n",
    "```\n",
    "<line1, \"What is your name?\">\n",
    "<line2, \"The name is Bond, James Bond.\">\n",
    "```\n",
    "\n",
    "The map function is defined such that it emits the pair <word, 1> for each word in the input. When we apply this to our input, we get the following list of key-value pairs.\n",
    "\n",
    "```\n",
    "<\"what\", 1>\n",
    "<\"is\", 1>\n",
    "<\"your\", 1>\n",
    "<\"name\", 1>\n",
    "<\"the\", 1>\n",
    "<\"name\", 1>\n",
    "<\"is\", 1>\n",
    "<\"bond\", 1>\n",
    "<\"james\", 1>\n",
    "<\"bond\", 1>\n",
    "```\n",
    "\n",
    "The shuffle then groups the values by their keys, such that all values for the same key are in a list.\n",
    "```\n",
    "<\"bond\", list(1, 1)>\n",
    "<\"is\", list(1, 1)>\n",
    "<\"james\", list(1)>\n",
    "<\"name\", list(1, 1)>\n",
    "<\"the\", list(1)>\n",
    "<\"what\", list(1)>\n",
    "<\"your\", list(1)>\n",
    "```\n",
    "\n",
    "As reduce function, we output one line for each key. The lines contain the current key and the sum of the values of that key.\n",
    "\n",
    "```\n",
    "bond 2\n",
    "is 1\n",
    "james 1\n",
    "name 2\n",
    "the 1\n",
    "what 1\n",
    "your 1\n",
    "```\n",
    "\n",
    "### Parallelization\n",
    "\n",
    "The design of MapReduce enables parallelization for every step of the computational process. The input can be read in chunks to parallelize the creation of the initial key-value pairs. For example, we could have multiple text files, each with 1000 lines that could be processed in parallel. The parallelism is limited by the throughput of the storage and makes more sense, if the data is distributed across multiple physical machines. \n",
    "\n",
    "map() can be applied to each key-value pair independently and the potential for parallelism is only limited by the amount of data. \n",
    "\n",
    "shuffle() can start as soon as the first key-value pair is processed by the map function. This reduces the waiting times, such that the shuffling is often finished directly after the last computations for map are finished.\n",
    "\n",
    "reduce() can run in parallel for different keys. Thus, the parallelism is only limited by the number of unique keys created by map(). Moreover, reduce() can already start, once all results for a key are available. This is where sorting by shuffle can help. If the results passed to reduce() are sorted, reduce can start processing for a key, once it sees results for the next key. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Hadoop\n",
    "\n",
    "[Apache Hadoop](https://hadoop.apache.org/) is an open source implementation of MapReduce. For many years, Hadoop was the standard solution for any MapReduce application and Hadoop is still relevant for many applications. All major cloud providers offer Hadoop clusters in their portfolio. Hadoop 2.0 implements MapReduce in an architecture with three layers. \n",
    "\n",
    "<img src=\"images/hadoop_architecture.png\" alt=\"Outline of MapReduce\" style=\"width: 400px;\"/>\n",
    "\n",
    "The lowest layer is the *Hadoop Distributed File System* (HDFS) that is in charge of the data management. *Yet Another Resource Negotiator* (YARN) is running on top of the file system. YARN manages the use of computational resources within a Hadoop cluster. Applications for data processing that want to use the Hadoop cluster are running on top of YARN. For example, such applications can be written with the Hadoop implmentation of MapReduce. However, due to the success of the HDFS and YARN, there are also other technologies that can be used for data processing, e.g., [Apache Spark](#Apache-Spark), which we discuss below. \n",
    "\n",
    "### HDFS\n",
    "\n",
    "The HDFS is the core of Hadoop. All data that should be analyzed is stored in the HDFS. HDFS was designed with the goal to enable Big Data processing, which is why HDFS behave quite differently from other file systems that we regularly use like NTFS, ext3, or xfs. \n",
    "\n",
    "- HDFS favors high throughput at the cost of low latency. This means that loading and storing large amounts of data is fast, but you may have to wait some time before the operation starts. \n",
    "- HDFS supports extremely large files. The file size is only limited by the amount of *distributed* storage, i.e., files can be large than the storage available at a single node. \n",
    "- HDFS is designed to support data local computations and minimize the data that needs to be send around in a cluster for file operations.\n",
    "- Since outages of single nodes in a large compute cluster is not a rare event, but rather a part of the daily work, HDFS is designed to be resilient against hardware failures, such that there is no loss of data and no interruption of service. \n",
    "\n",
    "In principle, HDFS uses a master/worker paradigm with a *NameNode* that manages *DataNodes*. \n",
    "\n",
    "<img src=\"images/hadoop_hdfs.png\" alt=\"Outline of MapReduce\" style=\"width: 600px;\"/>\n",
    "\n",
    "Clients access the HDFS via the NameNode. All file system operations, such as the creating, deletion, copying of a file is performed by requesting this at the NameNode. Whenever a file is created, it is split into smaller blocks. The NameNode organizes the creation, deletion, and replication of *blocks* on DataNodes. Replication means that each block is not just stored on a single DataNode, but on multiple DataNodes. This ensures that no data is lost, if a data node is lost. To avoid that the HDFS is not available if the NameNode crashes, there can also be a secondary NameNode. This avoids that the NameNode is a single point of failure. If there is a problem with the primary NameNode, the secondary NameNode can take over without loss of service. \n",
    "\n",
    "Another important aspect of the HDFS is that while users access the HDFS via the NameNode, the actual data is never send via the NameNode to the DataNodes, but directly from the users to the DataNodes. The following figure shows how a file in HDFS is created by a user. \n",
    "\n",
    "<img src=\"images/hadoop_writefile.png\" alt=\"Outline of MapReduce\" style=\"width: 700px;\"/>\n",
    "\n",
    "1. The user contacts the NameNode with the request to create a new file. \n",
    "2. The name node responds with a data stream that can be used for writing the file. From the users perspective, this is a normal file stream that would be used for local data access (e.g., `FileInputStream` in Java, `ifstream` in C++, `open` in Python).\n",
    "3. The user writes the contents of the file to the file stream. The NameNode configured the file stream with the information how the blocks should look like and where the data should be send. The data is directly send block-wise to one of the DataNodes. \n",
    "4. The DataNode that receives the block does not necesarily store the block itself. Instead, the block may be forwarded to other DataNodes for storage. Each block is stored at different nodes and the number of these nodes is defined by the *replication level*. Moreover, the HDFS ensures that blocks are evenly distributed among the data nodes, i.e., a DataNode only stores multiple blocks of a file, if the block cannot be stored at another DataNode. Ideally, all DataNodes store the same amount of blocks of a file.\n",
    "5. When a DataNode stores a block, this is acknowledged to the DataNode that receives the data from the user.\n",
    "6. Once the DataNode received acknowledgements for all replications of a block, the DataNode acknowledges to the user that the block is stored. The user can then start sending the data for the next block (goto step 3), until all blocks are written. The user does not observe this behavior directly, because this is automatically handled by the file stream. \n",
    "7. The user informs the NameNode that the writing finished and closes the file stream.\n",
    "\n",
    "### YARN\n",
    "\n",
    "<img src=\"images/hadoop_yarn.png\" alt=\"Outline of MapReduce\" style=\"width: 600px;\"/>\n",
    "\n",
    "<img src=\"images/hadoop_execution.png\" alt=\"Outline of MapReduce\" style=\"width: 600px;\"/>\n",
    "\n",
    "### MapReduce with Hadoop\n",
    "\n",
    "```java\n",
    "public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable> {\n",
    "    private final static IntWritable one = new IntWritable(1);\n",
    "    private Text word = new Text();\n",
    "    \n",
    "    public void map(Object key, Text value, Context context\n",
    "                   ) throws IOException, InterruptedException {\n",
    "        // text into tokens\n",
    "        StringTokenizer itr = new StringTokenizer(value.toString());\n",
    "        while (its.hasMoreTokens()) {\n",
    "            // add an output pair <word, 1> for each token\n",
    "            word.set(itr.nextToken());\n",
    "            context.write(word, one);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "```java\n",
    "public static class IntSumReader extends Reducer<Text, IntWritable, Text, IntWritable> {\n",
    "    private IntWritable result = new IntWritable();\n",
    "    \n",
    "    public void reduce(Text key, Iterable<IntWritable> values, Context context\n",
    "                       ) throws IOException, InterruptedException {\n",
    "        // calculate sum of word counts\n",
    "        int sum = 0;\n",
    "        for (IntWritable val : values) {\n",
    "            sum += val.get();\n",
    "        }\n",
    "        result.set(sum);\n",
    "        // write result\n",
    "        context.write(key, result);\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "```java\n",
    "public class WordCount {\n",
    "    public static void main(String[] args) throws Exception {\n",
    "        // Hadoop configuration\n",
    "        Configuration conf = new Configuration();\n",
    "        \n",
    "        // Create a Job with the name \"word count\"\n",
    "        Job job = Job.getInstance(conf, \"word count\");\n",
    "        job.setJahrByClass(WordCount.class);\n",
    "        \n",
    "        // set mapper, reducer, and output types\n",
    "        job.setMapperClass(TokenizerMapper.class);\n",
    "        job.setReduczer(IntSumReducer.class);\n",
    "        job.setOutputKeyClass(Text.class);\n",
    "        job.setOutputValueClass(IntWritable.class);\n",
    "        \n",
    "        // specify input and output files\n",
    "        FileInputFormat.addInputPath(job, new Path(args[0]));\n",
    "        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n",
    "        \n",
    "        // run job and wait for completion\n",
    "        job.waitForCompletion(true);\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### Word Count with Hadoop\n",
    "\n",
    "<img src=\"images/hadoop_wordcount_1.png\" alt=\"Hadoop Word Count (1)\" style=\"width: 600px;\"/>\n",
    "\n",
    "<img src=\"images/hadoop_wordcount_2.png\" alt=\"Hadoop Word Count (2)\" style=\"width: 600px;\"/>\n",
    "\n",
    "<img src=\"images/hadoop_wordcount_3.png\" alt=\"Hadoop Word Count (3)\" style=\"width: 600px;\"/>\n",
    "\n",
    "<img src=\"images/hadoop_wordcount_4.png\" alt=\"Hadoop Word Count (4)\" style=\"width: 600px;\"/>\n",
    "\n",
    "<img src=\"images/hadoop_wordcount_5.png\" alt=\"Hadoop Word Count (5)\" style=\"width: 600px;\"/>\n",
    "\n",
    "<img src=\"images/hadoop_wordcount_6.png\" alt=\"Hadoop Word Count (6)\" style=\"width: 600px;\"/>\n",
    "\n",
    "### Streaming Mode\n",
    "\n",
    "### Additional Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

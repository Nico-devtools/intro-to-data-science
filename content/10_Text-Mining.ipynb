{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining\n",
    "\n",
    "## Overview\n",
    "\n",
    "Text mining is the application of the techniques we discussed so far to textual data with the goal to infer information from the data. Examples for text mining applications are, e.g., the analysis of costumer reviews to infer their sentiment or the automated grouping of related documents. The problem with analyzing natural language text is that sentences or longer texts are neither numeric nor categorical data. Moreover, there is often some inherent structure in texts, e.g., headlines, introductions, references to other related content, or summaries. When we read text, we automatically identify these structures that textual data has internally. This is one of the biggest challenges of text mining: finding a good representation of the text such that it can be used for machine learning. \n",
    "\n",
    "For this, the text has to be somehow *encoded* into numeric or categorical data with as little loss of information as possible. The ideal encoding captures not only the words, but also the meaning of the words in their *context*, the grammatical structure, as well as the broader context of the text, e.g., of sentences within a document. To achieve this is still a subject of ongoing research. However, there were many advancements in recent years that made text mining into a powerful, versatile, and often sufficiently reliable tool. Since text mining itself is a huge field, we can only scratch the surface of the topic. The goal is that upon reading this chapter, you have a good idea the challenges of text mining, know basic text processing techniques, and also have a general idea of how more advanced text mining works.\n",
    "\n",
    "We will use the following eight tweets from Donald Trump as an example for textual data to demonstrate how text mining works in general. All data processing steps are done with the goal to prepare the text such that it is possible to analyze the topic of the tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oct 4, 2018 08:03:25 PM Beautiful evening in Rochester, Minnesota.\n",
      "VOTE, VOTE, VOTE! https://t.co/SyxrxvTpZE [Twitter for iPhone]\n",
      "\n",
      "Oct 4, 2018 07:52:20 PM Thank you Minnesota - I love you!\n",
      "https://t.co/eQC2NqdIil [Twitter for iPhone]\n",
      "\n",
      "Oct 4, 2018 05:58:21 PM Just made my second stop in Minnesota for a\n",
      "MAKE AMERICA GREAT AGAIN rally. We need to elect @KarinHousley to the\n",
      "U.S. Senate, and we need the strong leadership of @TomEmmer,\n",
      "@Jason2CD, @JimHagedornMN and @PeteStauber in the U.S. House! [Twitter\n",
      "for iPhone]\n",
      "\n",
      "Oct 4, 2018 05:17:48 PM Congressman Bishop is doing a GREAT job! He\n",
      "helped pass tax reform which lowered taxes for EVERYONE! Nancy Pelosi\n",
      "is spending hundreds of thousands of dollars on his opponent because\n",
      "they both support a liberal agenda of higher taxes and wasteful\n",
      "spending! [Twitter for iPhone]\n",
      "\n",
      "Oct 4, 2018 02:29:27 PM \"U.S. Stocks Widen Global Lead\"\n",
      "https://t.co/Snhv08ulcO [Twitter for iPhone]\n",
      "\n",
      "Oct 4, 2018 02:17:28 PM Statement on National Strategy for\n",
      "Counterterrorism: https://t.co/ajFBg9Elsj https://t.co/Qr56ycjMAV\n",
      "[Twitter for iPhone]\n",
      "\n",
      "Oct 4, 2018 12:38:08 PM Working hard, thank you!\n",
      "https://t.co/6HQVaEXH0I [Twitter for iPhone]\n",
      "\n",
      "Oct 4, 2018 09:17:01 AM This is now the 7th. time the FBI has\n",
      "investigated Judge Kavanaugh. If we made it 100, it would still not be\n",
      "good enough for the Obstructionist Democrats. [Twitter for iPhone]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textwrap import TextWrapper\n",
    "\n",
    "tweets_list = ['Oct 4, 2018 08:03:25 PM Beautiful evening in Rochester, Minnesota. VOTE, VOTE, VOTE! https://t.co/SyxrxvTpZE [Twitter for iPhone]',\n",
    "               'Oct 4, 2018 07:52:20 PM Thank you Minnesota - I love you! https://t.co/eQC2NqdIil [Twitter for iPhone]',\n",
    "               'Oct 4, 2018 05:58:21 PM Just made my second stop in Minnesota for a MAKE AMERICA GREAT AGAIN rally. We need to elect @KarinHousley to the U.S. Senate, and we need the strong leadership of @TomEmmer, @Jason2CD, @JimHagedornMN and @PeteStauber in the U.S. House! [Twitter for iPhone]',\n",
    "               'Oct 4, 2018 05:17:48 PM Congressman Bishop is doing a GREAT job! He helped pass tax reform which lowered taxes for EVERYONE! Nancy Pelosi is spending hundreds of thousands of dollars on his opponent because they both support a liberal agenda of higher taxes and wasteful spending! [Twitter for iPhone]',\n",
    "               'Oct 4, 2018 02:29:27 PM \"U.S. Stocks Widen Global Lead\" https://t.co/Snhv08ulcO [Twitter for iPhone]',\n",
    "               'Oct 4, 2018 02:17:28 PM Statement on National Strategy for Counterterrorism: https://t.co/ajFBg9Elsj https://t.co/Qr56ycjMAV [Twitter for iPhone]',\n",
    "               'Oct 4, 2018 12:38:08 PM Working hard, thank you! https://t.co/6HQVaEXH0I [Twitter for iPhone]',\n",
    "               'Oct 4, 2018 09:17:01 AM This is now the 7th. time the FBI has investigated Judge Kavanaugh. If we made it 100, it would still not be good enough for the Obstructionist Democrats. [Twitter for iPhone]']\n",
    "\n",
    "wrapper = TextWrapper(width=70)\n",
    "for tweet in tweets_list:\n",
    "    print('\\n'.join(wrapper.wrap(tweet)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Through preprocessing, text is transformed into a representation that we can use for machine learning algorithms, e.g., for the classification or for the grouping with clustering. \n",
    "\n",
    "### Creation of  a Corpus\n",
    "\n",
    "The first preprocessing step is to create a *corpus* of *documents*. In the sense of the terminology we have used so far, the documents are the objects that we want to reason about, the corpus is a collection of object. In our Twitter example, the corpus is a collection of tweets, and each tweet is a document. In our case, we already have a list of tweets, which is the same as a corpus of documents. In other use cases, this can be more difficult. For example, if you crawl the internet to collect reviews for a product, it is likely that you find multiple reviews on the same Web site. In this case, you must extract the reviews into separate documents, which can be challenging.\n",
    "\n",
    "### Relevant Content\n",
    "\n",
    "Textual data, especially text that was automatically collected from the Internet, often contains irrelevant content for a given use case. For example, if we only want to analyze the topic of tweets, the timestamps are irrelevant. It does also not matter if a tweet was sent with an iPhone or a different application. Links are a tricky case, as they may contain relevant information, but are also often irrelevant. For example, the URL of this page contains relevant information, e.g., the author, the general topic, and the name of the current chapter. Other parts, like the http are irrelevant. Other links are completely irrelevant, e.g., in case link shorteners are used. In this case a link is just a random string. \n",
    "\n",
    "When we strip the irrelevant content from the tweets, we get the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beautiful evening in Rochester, Minnesota. VOTE, VOTE, VOTE!\n",
      "\n",
      "Thank you Minnesota - I love you!\n",
      "\n",
      "Just made my second stop in Minnesota for a MAKE AMERICA GREAT AGAIN\n",
      "rally. We need to elect @KarinHousley to the U.S. Senate, and we need\n",
      "the strong leadership of @TomEmmer, @Jason2CD, @JimHagedornMN and\n",
      "@PeteStauber in the U.S. House!\n",
      "\n",
      "Congressman Bishop is doing a GREAT job! He helped pass tax reform\n",
      "which lowered taxes for EVERYONE! Nancy Pelosi is spending hundreds of\n",
      "thousands of dollars on his opponent because they both support a\n",
      "liberal agenda of higher taxes and wasteful spending!\n",
      "\n",
      "\"U.S. Stocks Widen Global Lead\"\n",
      "\n",
      "Statement on National Strategy for Counterterrorism:\n",
      "\n",
      "Working hard, thank you!\n",
      "\n",
      "This is now the 7th. time the FBI has investigated Judge Kavanaugh. If\n",
      "we made it 100, it would still not be good enough for the\n",
      "Obstructionist Democrats.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "tweets_relevant_content = []\n",
    "for tweet in tweets_list:\n",
    "    # remove the first 24 chars, because they are the time stamp\n",
    "    # remove everything after last [ because this is the source of the tweet\n",
    "    modified_tweet = tweet[24:tweet.rfind('[')]\n",
    "    # drop links\n",
    "    modified_tweet = re.sub(r'http\\S+', '', modified_tweet).strip()\n",
    "    tweets_relevant_content.append(modified_tweet)\n",
    "\n",
    "for tweet in tweets_relevant_content:\n",
    "    print('\\n'.join(wrapper.wrap(tweet)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is relevant and irrelevant can also depend on the context. For example, a different use case for Twitter data would be to analyze if tweets there a differences between tweets from different sources. In this case, the source cannot be dropped, but would be needed to divide the tweets by their source. Another analysis of Twitter data may want to consider how the content of tweets evolves over time. In this case, the timestamps cannot just be dropped. Therefore, every text mining application should carefully consider what is relevant and tailor the contents of the text to the specific needs. \n",
    "\n",
    "### Punctuation and Cases\n",
    "\n",
    "When we are only interested in the topic of documents, the punctuation, as well as the cases of the letters are often not useful and introduce unwanted differences between the same words. A relevant corner case of dropping punctuation and cases are acronyms. The acronym `U.S.` from the tweets is a perfect example for this, because this becomes `us`, which has a completely different meaning. If you are aware that the may be such problems within your data, you can manually address them, e.g., by mapping and `US` to `usa` after dropping the punctuation, but before lower casing the string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beautiful evening in rochester minnesota vote vote vote\n",
      "\n",
      "thank you minnesota  i love you\n",
      "\n",
      "just made my second stop in minnesota for a make america great again\n",
      "rally we need to elect karinhousley to the usa senate and we need the\n",
      "strong leadership of tomemmer jason2cd jimhagedornmn and petestauber\n",
      "in the usa house\n",
      "\n",
      "congressman bishop is doing a great job he helped pass tax reform\n",
      "which lowered taxes for everyone nancy pelosi is spending hundreds of\n",
      "thousands of dollars on his opponent because they both support a\n",
      "liberal agenda of higher taxes and wasteful spending\n",
      "\n",
      "usa stocks widen global lead\n",
      "\n",
      "statement on national strategy for counterterrorism\n",
      "\n",
      "working hard thank you\n",
      "\n",
      "this is now the 7th time the fbi has investigated judge kavanaugh if\n",
      "we made it 100 it would still not be good enough for the\n",
      "obstructionist democrats\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "tweets_lowercase = []\n",
    "\n",
    "for tweet in tweets_relevant_content:\n",
    "    modified_tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    modified_tweet = modified_tweet.replace('US', 'usa')\n",
    "    modified_tweet = modified_tweet.lower()\n",
    "    tweets_lowercase.append(modified_tweet)\n",
    "    \n",
    "for tweet in tweets_lowercase:\n",
    "    print('\\n'.join(wrapper.wrap(tweet)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beyond the Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "### Dimensionality\n",
    "\n",
    "### Ambiguities\n",
    "\n",
    "### Syntax and Semantic\n",
    "\n",
    "### Parsing"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

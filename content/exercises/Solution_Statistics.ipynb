{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics\n",
    "\n",
    "With this exercise, you can learn more about statistics and apply hypothesis testing, effect sizes, and determine the confidence interval of an machine learning experiment. \n",
    "\n",
    "## Data and Libraries\n",
    "\n",
    "Your task in this exercise is to apply statistical tests to compare the performance of two classification models. You can find everything you need in ```sklearn``` and ```scipy.stats```. For this exercise, we use the Iris data. The data is available in ```sklearn```. \n",
    "\n",
    "## Repeated Training with Repeated Sampling\n",
    "\n",
    "Train classification models with a 5-nearest neighbor classifier and random forest classifier (100 estimators) for the Iris data using  5 different randomized train/test-splits with 50% data as training data. Calculate Matthews Correlation Coefficient (MCC) for each of these classifiers and create two arrays: one with the MCC values of the nearest neighbor classifier and one for the random forest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Comparison\n",
    "\n",
    "Compare the summary statistics mean, standard deviation, median, min, and max of the estimates for the MCC of both classifiers. Use statistical tests to determine if there is a statistically significant difference between the classifiers. If the difference is significant, use Cohen's $d$ to estimate the effect size. Moreover, calculate the confidence interval for the mean value of MCC to estimate the reliability of your performance estimation.\n",
    "\n",
    "Repeat all of the above with 10, 50, and 100 train/test splits. How do the results depend on the number of repetitions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "estimating performances of KNN and random forest with 5\n",
      "train/test splits\n",
      "\n",
      "Random Forest: mean   = 0.9183 [0.9051,0.9315]\n",
      "               sd     = 0.0151\n",
      "               median = 0.9210\n",
      "               min    = 0.9022\n",
      "               max    = 0.9423\n",
      "KNN:           mean   = 0.9377 [0.9187,0.9566]\n",
      "               sd     = 0.0216\n",
      "               median = 0.9403\n",
      "               min    = 0.9061\n",
      "               max    = 0.9610\n",
      "\n",
      "p-value of Shapiro-Wilk test for random forest data:   0.3981\n",
      "The test found that the data sample was normal, failing to reject\n",
      "the null hypothesis at significance level alpha=0.005\n",
      "\n",
      "p-value of Shapiro-Wilk test for KNN data: 0.4484\n",
      "The test found that the data sample was normal, failing to reject\n",
      "the null hypothesis at significance level alpha=0.005\n",
      "\n",
      "Both populations normal. Using Welch's t-test.\n",
      "\n",
      "p-value of Welch's t-tests: 0.183827\n",
      "The test found that the population means are equal, failing to\n",
      "reject the null hypothesis at significance level alpha=0.005\n",
      "----------------------------------------------------------------\n",
      "estimating performances of KNN and random forest with 10\n",
      "train/test splits\n",
      "\n",
      "Random Forest: mean   = 0.9233 [0.9066,0.9401]\n",
      "               sd     = 0.0331\n",
      "               median = 0.9210\n",
      "               min    = 0.8658\n",
      "               max    = 0.9803\n",
      "KNN:           mean   = 0.9478 [0.9369,0.9587]\n",
      "               sd     = 0.0215\n",
      "               median = 0.9600\n",
      "               min    = 0.9061\n",
      "               max    = 0.9803\n",
      "\n",
      "p-value of Shapiro-Wilk test for random forest data:   0.2464\n",
      "The test found that the data sample was normal, failing to reject\n",
      "the null hypothesis at significance level alpha=0.005\n",
      "\n",
      "p-value of Shapiro-Wilk test for KNN data: 0.1937\n",
      "The test found that the data sample was normal, failing to reject\n",
      "the null hypothesis at significance level alpha=0.005\n",
      "\n",
      "Both populations normal. Using Welch's t-test.\n",
      "\n",
      "p-value of Welch's t-tests: 0.029240\n",
      "The test found that the population means are equal, failing to\n",
      "reject the null hypothesis at significance level alpha=0.005\n",
      "----------------------------------------------------------------\n",
      "estimating performances of KNN and random forest with 50\n",
      "train/test splits\n",
      "\n",
      "Random Forest: mean   = 0.9255 [0.9184,0.9326]\n",
      "               sd     = 0.0291\n",
      "               median = 0.9210\n",
      "               min    = 0.8658\n",
      "               max    = 0.9803\n",
      "KNN:           mean   = 0.9386 [0.9318,0.9454]\n",
      "               sd     = 0.0281\n",
      "               median = 0.9403\n",
      "               min    = 0.8222\n",
      "               max    = 1.0000\n",
      "\n",
      "p-value of Shapiro-Wilk test for random forest data:   0.0211\n",
      "The test found that the data sample was normal, failing to reject\n",
      "the null hypothesis at significance level alpha=0.005\n",
      "\n",
      "p-value of Shapiro-Wilk test for KNN data: 0.0005\n",
      "The test found that the data sample was not normal, rejecting the\n",
      "null hypothesis at significance level alpha=0.005\n",
      "\n",
      "At least one population not normal. Using Mann-Whitney-U test.\n",
      "\n",
      "p-value of Mann-Whitney-U test: 0.002456\n",
      "The test found that the population means are not equal, rejecting\n",
      "the null hypothesis at significance level alpha=0.005\n",
      "Effect size (Cohen's d): -0.454 - small effect\n",
      "----------------------------------------------------------------\n",
      "estimating performances of KNN and random forest with 100\n",
      "train/test splits\n",
      "\n",
      "Random Forest: mean   = 0.9255 [0.9213,0.9297]\n",
      "               sd     = 0.0277\n",
      "               median = 0.9210\n",
      "               min    = 0.8602\n",
      "               max    = 0.9803\n",
      "KNN:           mean   = 0.9411 [0.9374,0.9448]\n",
      "               sd     = 0.0242\n",
      "               median = 0.9403\n",
      "               min    = 0.8222\n",
      "               max    = 1.0000\n",
      "\n",
      "p-value of Shapiro-Wilk test for random forest data:   0.0002\n",
      "The test found that the data sample was not normal, rejecting the\n",
      "null hypothesis at significance level alpha=0.005\n",
      "\n",
      "p-value of Shapiro-Wilk test for KNN data: 0.0000\n",
      "The test found that the data sample was not normal, rejecting the\n",
      "null hypothesis at significance level alpha=0.005\n",
      "\n",
      "At least one population not normal. Using Mann-Whitney-U test.\n",
      "\n",
      "p-value of Mann-Whitney-U test: 0.000000\n",
      "The test found that the population means are not equal, rejecting\n",
      "the null hypothesis at significance level alpha=0.005\n",
      "Effect size (Cohen's d): -0.600 - medium effect\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy import stats\n",
    "from statistics import mean, stdev\n",
    "from math import sqrt\n",
    "from scipy.stats import norm\n",
    "from textwrap import TextWrapper\n",
    "\n",
    "wrapper = TextWrapper(width=65)\n",
    "\n",
    "\n",
    "def wrap_print(string):\n",
    "    print('\\n'.join(wrapper.wrap(string)))\n",
    "\n",
    "\n",
    "X, Y = load_iris(return_X_y=True)\n",
    "\n",
    "knn = KNeighborsClassifier(5)\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "scores_rf = []\n",
    "scores_knn = []\n",
    "for repetitions in [5, 10, 50, 100]:\n",
    "    wrap_print(\"----------------------------------------------------------------\")\n",
    "    wrap_print(\n",
    "        \"estimating performances of KNN and random forest with %i train/test splits\" % repetitions)\n",
    "    for i in range(0, repetitions):\n",
    "        # bootstrap loop\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "            X, Y, test_size=0.50, stratify=Y)\n",
    "        rf.fit(X_train, Y_train)\n",
    "        Y_pred = rf.predict(X_test)\n",
    "        scores_rf.append(matthews_corrcoef(Y_test, Y_pred))\n",
    "        knn.fit(X_train, Y_train)\n",
    "        Y_pred = knn.predict(X_test)\n",
    "        scores_knn.append(matthews_corrcoef(Y_test, Y_pred))\n",
    "\n",
    "    s1 = scores_rf\n",
    "    s2 = scores_knn\n",
    "\n",
    "    def calc_ci(mu, sd, n, cl):\n",
    "        dev = norm.ppf((1-cl)/2)*sd/sqrt(n)\n",
    "        return mu+dev, mu-dev\n",
    "\n",
    "    s1_lower, s1_upper = calc_ci(np.mean(s1), np.std(s1), len(s1), 0.95)\n",
    "    s2_lower, s2_upper = calc_ci(np.mean(s2), np.std(s2), len(s2), 0.95)\n",
    "\n",
    "    print()\n",
    "    print(\"Random Forest: mean   = %.4f [%.4f,%.4f]\" % (\n",
    "        np.mean(s1), s1_lower, s1_upper))\n",
    "    print(\"               sd     = %.4f\" % np.std(s1))\n",
    "    print(\"               median = %.4f\" % np.median(s1))\n",
    "    print(\"               min    = %.4f\" % np.min(s1))\n",
    "    print(\"               max    = %.4f\" % np.max(s1))\n",
    "    print(\"KNN:           mean   = %.4f [%.4f,%.4f]\" % (\n",
    "        np.mean(s2), s2_lower, s2_upper))\n",
    "    print(\"               sd     = %.4f\" % np.std(s2))\n",
    "    print(\"               median = %.4f\" % np.median(s2))\n",
    "    print(\"               min    = %.4f\" % np.min(s2))\n",
    "    print(\"               max    = %.4f\" % np.max(s2))\n",
    "    print()\n",
    "\n",
    "    alpha = 0.005\n",
    "    pval_shapiro1 = stats.shapiro(s1)[1]\n",
    "    pval_shapiro2 = stats.shapiro(s2)[1]\n",
    "\n",
    "    print('p-value of Shapiro-Wilk test for random forest data:   %.4f' %\n",
    "          pval_shapiro1)\n",
    "    if pval_shapiro1 > alpha:\n",
    "        wrap_print('The test found that the data sample was normal, failing to reject the null hypothesis at significance level alpha=%.3f' % alpha)\n",
    "    else:\n",
    "        wrap_print(\n",
    "            'The test found that the data sample was not normal, rejecting the null hypothesis at significance level alpha=%.3f' % alpha)\n",
    "\n",
    "    print()\n",
    "    print('p-value of Shapiro-Wilk test for KNN data: %.4f' % pval_shapiro2)\n",
    "    if pval_shapiro2 > alpha:\n",
    "        wrap_print('The test found that the data sample was normal, failing to reject the null hypothesis at significance level alpha=%.3f' % alpha)\n",
    "    else:\n",
    "        wrap_print(\n",
    "            'The test found that the data sample was not normal, rejecting the null hypothesis at significance level alpha=%.3f' % alpha)\n",
    "    print()\n",
    "\n",
    "    if pval_shapiro1 > alpha and pval_shapiro2 > alpha:\n",
    "        wrap_print(\"Both populations normal. Using Welch's t-test.\")\n",
    "        pval_equal = stats.ttest_ind(s1, s2, equal_var=False)[1]\n",
    "        print()\n",
    "        print(\"p-value of Welch's t-tests: %f\" % pval_equal)\n",
    "    else:\n",
    "        wrap_print(\n",
    "            \"At least one population not normal. Using Mann-Whitney-U test.\")\n",
    "        pval_equal = stats.mannwhitneyu(s1, s2, alternative='two-sided')[1]\n",
    "        print()\n",
    "        print(\"p-value of Mann-Whitney-U test: %f\" % pval_equal)\n",
    "\n",
    "    if pval_equal > alpha:\n",
    "        wrap_print('The test found that the population means are equal, failing to reject the null hypothesis at significance level alpha=%.3f' % alpha)\n",
    "    else:\n",
    "        wrap_print('The test found that the population means are not equal, rejecting the null hypothesis at significance level alpha=%.3f' % alpha)\n",
    "        # test conditions\n",
    "        s = sqrt(((len(s1)-1)*stdev(s1)**2 + (len(s2)-1)\n",
    "                  * stdev(s2)**2)/(len(s1)+len(s2)-2))\n",
    "        cohens_d = (mean(s1) - mean(s2)) / s\n",
    "\n",
    "        if abs(cohens_d) < 0.01:\n",
    "            effsizestr = \"negligible\"\n",
    "        elif abs(cohens_d) < 0.2:\n",
    "            effsizestr = \"very small\"\n",
    "        elif abs(cohens_d) < 0.5:\n",
    "            effsizestr = \"small\"\n",
    "        elif abs(cohens_d) < 0.8:\n",
    "            effsizestr = \"medium\"\n",
    "        elif abs(cohens_d) < 1.2:\n",
    "            effsizestr = \"large\"\n",
    "        elif abs(cohens_d) < 2:\n",
    "            effsizestr = \"very large\"\n",
    "        else:\n",
    "            effsizestr = \"huge\"\n",
    "\n",
    "        wrap_print(\"Effect size (Cohen's d): %.3f - %s effect\" %\n",
    "                   (cohens_d, effsizestr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that while the statistical markers are fairly stable, the results of the statistics changes with an increasing number of iterations. With five and ten iterations, we do not find a statistically significant difference. We also observe that the confidence interval is fairly large, with a range of almost 5%. Once the sample size is increases to 50 and 100, we can observe that the difference between the two algorithms is significant. Similarly, we see that the confidence interval shrinks with increasing data size to a range of less than 1% with 100 samples.\n",
    "\n",
    "This demonstrate the most important aspect of any data analysis: never trust results with very few samples. \n",
    "\n",
    "Regarding the effect size, we observe that Cohen's $d$ can also be misleading. We see that the effect size is medium with $d \\approx 0.7$. However, the actual difference between the mean values is less than 2%, i.e., the performance of both classifiers is very similar. Regardless, the effect size is relatively large, because the standard deviation is very small. This demonstrates that effect sizes, while effective, should be used with care, especially if the standard deviation is very small. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

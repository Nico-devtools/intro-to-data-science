---
redirect_from:
  - "/10-text-mining"
interact_link: content/10_Text-Mining.ipynb
kernel_name: python3
kernel_path: content
has_widgets: false
title: |-
  Text Mining
pagenum: 10
prev_page:
  url: /09_Time-Series-Analysis.html
next_page:
  url: /Exercises.html
suffix: .ipynb
search: text mining oct twitter iphone pm data t co words textual e g challenges good made minnesota vote u s techniques goal infer information related sentences texts numeric categorical often structure content into not only also context document still idea general works thank stop great need taxes spending bag overview application discussed far examples applications analysis costumer reviews sentiment automated grouping documents problem analyzing natural language longer neither nor moreover inherent headlines introductions references summaries read automatically identify structures internally biggest finding representation such used machine learning somehow encoded little loss possible ideal encoding captures meaning grammatical well broader within

comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

    <main class="jupyter-page">
    <div id="page-info"><div id="page-title">Text Mining</div>
</div>
    
<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Overview">Overview<a class="anchor-link" href="#Overview"> </a></h2><p>Text mining is the application of the techniques we discussed so far to textual data with the goal to infer information from the data. Examples for text mining applications are, e.g., the analysis of costumer reviews to infer their sentiment or the automated grouping of related documents. The problem with analyzing natural language text is that sentences or longer texts are neither numeric nor categorical data. Moreover, there is often some inherent structure in texts, e.g., headlines, introductions, references to other related content, or summaries. When we read text, we automatically identify these structures that textual data has internally. This is one of the biggest challenges of text mining: finding a good representation of the text such that it can be used for machine learning.</p>
<p>For this, the text has to be somehow <em>encoded</em> into numeric or categorical data with as little loss of information as possible. The ideal encoding captures not only the words, but also the meaning of the words in their <em>context</em>, the grammatical structure, as well as the broader context of the text, e.g., of sentences within a document. To achieve this is still a subject of ongoing research. However, there were many advancements in recent years that made text mining into a powerful, versatile, and often sufficiently reliable tool. Since text mining itself is a huge field, we can only scratch the surface of the topic. The goal is that upon reading this chapter, you have a good idea the challenges of text mining, know basic text processing techniques, and also have a general idea of how more advanced text mining works.</p>
<p>We will use the following eight tweets from Donald Trump as an example for textual data to demonstrate how text mining works in general.</p>

<pre><code>Oct 4, 2018 08:03:25 PM Beautiful evening in Rochester, Minnesota. VOTE, VOTE, VOTE! https://t.co/SyxrxvTpZE [Twitter for iPhone] 

Oct 4, 2018 07:52:20 PM Thank you Minnesota - I love you! https://t.co/eQC2NqdIil [Twitter for iPhone] 

Oct 4, 2018 05:58:21 PM Just made my second stop in Minnesota for a MAKE AMERICA GREAT AGAIN rally. We need to elect @KarinHousley to the U.S. Senate, and we need the strong leadership of @TomEmmer, @Jason2CD, @JimHagedornMN and @PeteStauber in the U.S. House! [Twitter for iPhone] 

Oct 4, 2018 05:17:48 PM Congressman Bishop is doing a GREAT job! He helped pass tax reform which lowered taxes for EVERYONE! Nancy Pelosi is spending hundreds of thousands of dollars on his opponent because they both support a liberal agenda of higher taxes and wasteful spending! [Twitter for iPhone] 

Oct 4, 2018 02:29:27 PM “U.S. Stocks Widen Global Lead” https://t.co/Snhv08ulcO [Twitter for iPhone] 

Oct 4, 2018 02:17:28 PM Statement on National Strategy for Counterterrorism: https://t.co/ajFBg9Elsj https://t.co/Qr56ycjMAV [Twitter for iPhone] 

Oct 4, 2018 12:38:08 PM Working hard, thank you! https://t.co/6HQVaEXH0I [Twitter for iPhone] 

Oct 4, 2018 09:17:01 AM This is now the 7th. time the FBI has investigated Judge Kavanaugh. If we made it 100, it would still not be good enough for the Obstructionist Democrats. [Twitter for iPhone]</code></pre>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Preprocessing">Preprocessing<a class="anchor-link" href="#Preprocessing"> </a></h2><h3 id="Creation-of--a-Corpus">Creation of  a Corpus<a class="anchor-link" href="#Creation-of--a-Corpus"> </a></h3><h3 id="Relevant-Content">Relevant Content<a class="anchor-link" href="#Relevant-Content"> </a></h3><h3 id="Punctuation-and-Cases">Punctuation and Cases<a class="anchor-link" href="#Punctuation-and-Cases"> </a></h3><h3 id="Stop-Words">Stop Words<a class="anchor-link" href="#Stop-Words"> </a></h3><h3 id="Stemming-and-Lemmatization">Stemming and Lemmatization<a class="anchor-link" href="#Stemming-and-Lemmatization"> </a></h3><h3 id="Bag-of-Words">Bag-of-Words<a class="anchor-link" href="#Bag-of-Words"> </a></h3><h3 id="Inverse-Document-Frequency">Inverse Document Frequency<a class="anchor-link" href="#Inverse-Document-Frequency"> </a></h3><h3 id="Beyond-the-Bag-of-Words">Beyond the Bag-of-Words<a class="anchor-link" href="#Beyond-the-Bag-of-Words"> </a></h3>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Challenges">Challenges<a class="anchor-link" href="#Challenges"> </a></h2><h3 id="Dimensionality">Dimensionality<a class="anchor-link" href="#Dimensionality"> </a></h3><h3 id="Ambiguities">Ambiguities<a class="anchor-link" href="#Ambiguities"> </a></h3><h3 id="Syntax-and-Semantic">Syntax and Semantic<a class="anchor-link" href="#Syntax-and-Semantic"> </a></h3><h3 id="Parsing">Parsing<a class="anchor-link" href="#Parsing"> </a></h3>
</div>
</div>
</div>
</div>

 


    </main>
    